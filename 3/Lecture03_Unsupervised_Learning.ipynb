{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62a3e1ce",
   "metadata": {},
   "source": [
    "# <center>CS559 - Lecture 3: Unsupervised Learning </center>\n",
    "<center><b>Spring 2022</b></center>\n",
    "\n",
    "<p><a name=\"Outline\"></a></p>\n",
    "\n",
    "# Outline\n",
    "1. <a href=\"#RecapFromLastLecture\">Recap From Last Lecture </a><br>\n",
    "2. <a href=\"#Introduction to Unsupervised Learning\">Introduction to Unsupervised Learning</a><br>\n",
    "3. <a href=\"#Cluster Analysis\">Cluster Analysis</a><br>\n",
    "    1. <a href=\"#Kmeans\">K-means</a><br>\n",
    "    2. <a href=\"#Hierarchical\">Hierarchical</a><br>\n",
    "4. <a href=\"#PCA\">Dimentional Reduction: Principal Component Analysis (PCA)</a><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68407929",
   "metadata": {},
   "source": [
    "<p><a name=\"RecapFromLastLecture\"></a></p>\n",
    "\n",
    "# 1. Recap From Last Lecture\n",
    "## Machine Learning Overview\n",
    "- Machine Learning\n",
    "- Learning - Supervised Learning, Unsupervised Learning, Reinforcement Learning\n",
    "- Preprocessing - EDA, Scaling, Missing value imputation, Vectorizing categorical data, and Feature Engineering\n",
    "- Modeling - Data Split\n",
    "    - Training and Test sets\n",
    "    - Traning, Validation, and Test sets\n",
    "    - Cross Validation\n",
    "        - Heldout, K-Fold, etc... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3238a2a",
   "metadata": {},
   "source": [
    "<p><a name=\"Introduction to Unsupervised Learning\"></a></p>\n",
    "\n",
    "# 2. Introduction to Unsupervised Learning\n",
    "- Only a set of N observations with p features and no reponse (target) variables.\n",
    "    - Unlabeled data which is much easier to obtain... \n",
    "- Goal: to infer the properties directly without knowing the \"correct answers or the error for each observation. \n",
    "    - \"<b>Learning Without Teacher</b>\" - No direct measure of success.\n",
    "    - Therefore, more subjective than supervised learning.\n",
    "- Two methods:\n",
    "    - **Clustering**: a broad class of methods for grouping or segmenting a collection of objects into distinct subjects known as **clusters**.\n",
    "        - example: groups of online shoppers characterized by their browsing and purchase histories.\n",
    "    - **Dimension Reduction**: a method to reduce the dimension of data while keeping the most of information. \n",
    "        - often used for data visualization or data preprocessing for supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890fd01a",
   "metadata": {},
   "source": [
    "# 3. Cluster Analysis\n",
    "- Task: aim to uncover <b>underlying structure</b> of the data and see what pattern exists in the data. \n",
    "    - We aim to group together observations that are similar while separating observations that are dissimilar. \n",
    "- Cluster analysis attempts to explore possible subpopulations that exist within your data.\n",
    "- Typical questions that cluster analysis attempts to answer are: \n",
    "    - Approximately how many subgroups exist in the data? \n",
    "    - Approximately what are the sizes of the subgroups in the data? \n",
    "    - What commonalities exist among members in similar subgroups? \n",
    "    - Are there smaller subgroups that can further segment current subgroups? \n",
    "    - Are there any outlying observations? \n",
    "        - Notice that these questions are largely exploratory in nature. \n",
    "        \n",
    "## 3.A K-Means Clustering\n",
    "- With the K-means clustering algorithm, we aim to split up our observations into a predetermined number of clusters.\n",
    "    - The number of clusters K must be specified in advance.\n",
    "    - These cluster memberships are distinct and non-overlapping.\n",
    "- The data points in each of the clusters are determined to be mostly similar to a specific centroid value:\n",
    "    - The centroid of a cluster represents the average of the observations within a given cluster; it is a single theoretical center that represents the prototypical member that exists within the given cluster. \n",
    "    - Each observation will be assigned to exactly one of the K clusters depending on where the observation falls in the feature space relative to the cluster centroid locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36581be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e4248",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 8, 6\n",
    "np.random.seed(42)\n",
    "x1 = np.random.randn(100, 2) * 7 + 10 \n",
    "x2 = np.random.randn(100, 2) * 7 - 10 \n",
    "x = np.row_stack([x1, x2])\n",
    "plt.scatter(x[:, 0], x[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa41a49",
   "metadata": {},
   "source": [
    "- A simulated data set with 150 observation in 2-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa76ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8562e144",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,5))\n",
    "for i,ax in zip(range(2,5),[ax1, ax2, ax3]):\n",
    "    kmeans = KMeans(n_clusters=i)\n",
    "    kmeans.fit(x)\n",
    "    ax.scatter(x[:, 0], x[:, 1],c=kmeans.labels_, alpha=0.8)\n",
    "    ax.set_xlabel('x1')\n",
    "    ax.set_ylabel('x2')\n",
    "    ax.set_title('$k=$'+str(i))\n",
    "plt.subplots_adjust(left=0.1,bottom=0.1,right=1,top=0.9,wspace=0.4,hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff2021",
   "metadata": {},
   "source": [
    "- The color labels the cluster to which it has been assigned. Note that the cluster coloring is arbitrary since there is no absolute ordering of the clusters. \n",
    "- Now the main question: what technique does k-means algorithm use to create these clusters? \n",
    "    - Suppose we use the **Euclidean** distance \n",
    "    $$D(\\vec{q},\\vec{p})=\\sqrt{\\sum_{i=1}^N(q_i-p_i)^2}.$$ \n",
    "    - Then the **within-cluster variation** $W(C_k)$ is defined as: \n",
    "$$W(C_k)=\\frac{1}{|C_k|}\\sum_{i,i'\\in C_k}\\sum_{j=1}^p (x_{ij}-x_{i'j})^2$$\n",
    "    - where \n",
    "        - $C_k$ is the total number of observations in cluster $k$\n",
    "        - $i,i'\\in C_k$ are indices of observations in cluster $C_k$\n",
    "        - $p$ is the number of variables/features in dataset.\n",
    "- Since the within-cluster variation is a quantitative gauge of the amount by which the observations in a specific cluster differ from one another, we want to **minimize** the sum of this quantity $W(C_k)$ over all clusters: \n",
    "$$\\min_{C_1,\\dots,C_k}\\Big\\{\\sum_{i=1}^k W(C_i)\\Big\\}$$\n",
    "- In other words, we would like to partition the observations into K clusters such that the total within-cluster variation aggregated across all K clusters is as small as possible; the optimization problem for K-means is as follows:\n",
    "$$\\min_{C_1,\\dots,C_k}\\Big\\{\\sum_{i=1}^k \\frac{1}{|C_k|}\\sum_{i,i'\\in C_k}\\sum_{j=1}^p (x_{ij}-x_{i'j})^2\\Big\\}$$\n",
    "- To find the global minimum of the above function is very difficult.\n",
    "- In practice, most K-means packages perform the following greedy algorithm, also known as Lloyd algorithm in the computer science circle:\n",
    "    1. Randomly assign  an integer label, from 1 to K (where K is the number of clusters), to each of the observations. These serve as initial cluster assignments for the observations. \n",
    "    2. Iterate until the cluster assignments stop changing:\n",
    "        1. For each of the K clusters, compute the cluster’s new centroid. \n",
    "        2. Assign each observation to the cluster whose centroid is closest (closest is measured using Euclidean distance). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72217a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "x3 = np.random.randint(2, size=len(x1))\n",
    "x=pd.DataFrame(np.column_stack([x1,x2,x3]))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f0d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,5))\n",
    "\n",
    "x=np.row_stack([x1,x2])\n",
    "ax1.scatter(x[:,0], x[:,1])\n",
    "ax1.set_title('1. Data')\n",
    "\n",
    "np.random.seed(123)\n",
    "L=4\n",
    "x3 = np.random.randint(L, size=len(x))\n",
    "x=pd.DataFrame(np.column_stack([x,x3]))\n",
    "x.columns = ['x','y','label']\n",
    "\n",
    "\n",
    "\n",
    "x_mean=x.groupby('label').mean()\n",
    "x_mean.reset_index(inplace=True)\n",
    "\n",
    "for l,color in zip(range(0,L),['blue','red','green','yellow']):\n",
    "    ax2.scatter(x['x'][x['label']==l], x['y'][x['label']==l],alpha=0.5,color=color,label='label'+str(l))\n",
    "    ax2.legend()\n",
    "    ax2.set_title('2. Random Label Initialization')\n",
    "    ax3.scatter(x['x'][x['label']==l], x['y'][x['label']==l],alpha=0.5,color=color,label='label'+str(l))\n",
    "    ax3.scatter(x['x'][x['label']==l].mean(),x['y'][x['label']==l].mean(), marker=\"o\", \n",
    "                s=300,alpha=1,color=color,edgecolor='black')\n",
    "    ax3.legend()\n",
    "ax3.set_title('3. Centroids with the initial labels')\n",
    "plt.subplots_adjust(left=0.1,bottom=0.1,right=1,top=0.9,wspace=0.4,hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d3bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,5))\n",
    "kmeans = KMeans(n_clusters=L,random_state=0,n_init=1,max_iter=1)\n",
    "kmeans.fit(x[['x','y']])\n",
    "x['newlabel']=kmeans.labels_\n",
    "x['newlabel'].replace({3:1,0:2,2:0,1:3},inplace=True)\n",
    "for l,color in zip(range(0,L),['blue','red','green','yellow']):\n",
    "    ax1.scatter(x['x'][x['newlabel']==l], x['y'][x['newlabel']==l],alpha=0.5,color=color,label='new label'+str(l))\n",
    "    ax1.legend()\n",
    "    ax1.set_title('4. Cluster Respects to the Current Centroids')\n",
    "\n",
    "    ax2.scatter(x['x'][x['newlabel']==l], x['y'][x['newlabel']==l],alpha=0.5,color=color,label='new label'+str(l))\n",
    "    ax2.scatter(x['x'][x['newlabel']==l].mean(),x['y'][x['newlabel']==l].mean(), marker=\"o\", \n",
    "            s=300, color=color,alpha=1,edgecolors='black')\n",
    "    ax2.set_title('5. Find New Centroids')\n",
    "    ax2.legend()\n",
    "\n",
    "kmeans = KMeans(n_clusters=L,random_state=0,n_init=1,max_iter=10)\n",
    "kmeans.fit(x[['x','y']])\n",
    "x['newlabel']=kmeans.labels_\n",
    "x['newlabel'].replace({3:1,0:2,2:0,1:3},inplace=True)\n",
    "\n",
    "for l,color in zip(range(0,L),['blue','red','green','yellow']):\n",
    "    ax3.scatter(x['x'][x['newlabel']==l], x['y'][x['newlabel']==l],alpha=0.5,color=color,label='new label'+str(l))\n",
    "    ax3.scatter(x['x'][x['newlabel']==l].mean(),x['y'][x['newlabel']==l].mean(), marker=\"o\", \n",
    "            s=300, color=color,alpha=1,edgecolors='black')\n",
    "    ax3.set_title('6. Final Results')\n",
    "    ax3.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d991f7cf",
   "metadata": {},
   "source": [
    "- The K-means procedure always converges:\n",
    "    1. If you run the algorithm from a fixed initial assignment, it will reach a stable endpoint where the clustering solution will no longer change through the iterations. \n",
    "    2. Unfortunately, the guaranteed convergence is to a local minimum. \n",
    "        - Thus, if we begin the K-means algorithm with a different initial configuration, it is possible that convergence will find different centroids and therefore ultimately assigning different cluster memberships. \n",
    "- What can we do to get around this?\n",
    "    - Run the K-means procedure several times and pick the clustering solution that yields the smallest aggregate within-cluster variance. \n",
    "- The Kmeans++ (2007) improves the random seeding of the original KMeans.\n",
    "    1. The initialization step runs inductively.\n",
    "    2. Firstly, pick a data point randomly as the first centroid.\n",
    "    3. Suppose that k of the seed centroid have been chosen, compute for each data point x the distance $D(x)$ to the closest centroid among these k seed centroids. Select the $(k+1)^{th}$ centroid randomly, according to a probability distribution with probability proportional to $D(x)^2$.\n",
    "    4. In each inductive step, the newly found seed centroid trends to keep a far distance from the existing ones.\n",
    "    5. The algorithm will stop when it has found the least/best (local optimum) value. \n",
    "- The default initialization scheme of Scikit-Learn’s KMeans uses KMeans++. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d69f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 3, figsize=(15,10))\n",
    "max_iter = 10\n",
    "n_init=1\n",
    "for i in range(0,2):\n",
    "    for j in range(0,3):\n",
    "        n_init=np.random.randint(n_init,5)\n",
    "        max_iter = np.random.randint(max_iter,100)\n",
    "        kmeans = KMeans(n_clusters=4,n_init=n_init,max_iter=max_iter)\n",
    "        kmeans.fit(x[['x','y']])\n",
    "        axs[i,j].scatter(x['x'],x['y'],c=kmeans.labels_,alpha=0.8)\n",
    "        axs[i,j].set_title('inertia='+str(round(kmeans.inertia_,2))+', n_iter='+str(kmeans.n_iter_))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9adf4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inertia(km, X, n_cluster_range):\n",
    "    inertias = []\n",
    "    for i in n_cluster_range:\n",
    "        km.set_params(n_clusters=i)\n",
    "        km.fit(X)\n",
    "        inertias.append(km.inertia_)\n",
    "    plt.plot(n_cluster_range, inertias, marker='o')\n",
    "    plt.title('Elbow method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Inertia')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c37000",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inertia(KMeans(), x, range(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625d2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedbed69",
   "metadata": {},
   "source": [
    "**Arguments**:\n",
    "- n_clusters: The number of clusters to divide, default is 8.\n",
    "- max_iter: The maximal number of iterations, default is 300.\n",
    "- n_init: Number of time the k-means algorithm will run with different centroid seeds. The final results will be the best output of n_init consecutive runs in terms of inertia. default is 10.\n",
    "- init: Method for initialization, defaults to 'k-means++'. Other options are 'random' or an ndarray of shape (n_clusters, n_features) and gives the initial centers.\n",
    "- random_state: Optional. The generator used to initialize the centers. If an integer is given, it fixes the seed. Defaults to the global numpy random number generator.\n",
    "Usually, we just need to set the argument n_clusters to determine how many groups are we going to split.\n",
    "\n",
    "**Attributes**:\n",
    "- cluster_centers_: The coordinates of cluster centers.\n",
    "- labels_: Labels of each observation, which indicate the group number of each observation.\n",
    "- inertia_: Sum of distances of samples to their closest cluster center.\n",
    "\n",
    "The most import attribute here is the labels_ .\n",
    "\n",
    "**Methods**:\n",
    "- fit: Fit k-means clustering on a given data set.\n",
    "- fit_predict: Compute cluster centers and predict cluster index for each sample.\n",
    "- get_params: Get parameters for this estimator.\n",
    "- set_params: Set the parameters of this estimator.\n",
    "- predict: Given a set of data, predict the closest cluster each sample belongs to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167d22cd",
   "metadata": {},
   "source": [
    "In this case, we try to split the iris data into multiple groups by using the features sepal length, sepal width, petal length, petal width.\n",
    "- Set the argument n_clusters to 3.\n",
    "- Fit the iris data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a247caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d0645",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de41700",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_inertia(kmeans, iris.data, range(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a781626",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.set_params(n_clusters=3)\n",
    "kmeans.fit(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fcc470",
   "metadata": {},
   "source": [
    "- The label of each observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd18c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89101df",
   "metadata": {},
   "source": [
    "- The centroid of each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1855fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42fd321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import  matplotlib.pyplot as plt\n",
    "plt.scatter(iris.data[:, 2], iris.data[:, 3], c=kmeans.labels_, alpha=0.8)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 2], kmeans.cluster_centers_[:, 3], marker=\"+\", s=1000, c=[0, 1, 2])\n",
    "plt.xlabel('Petal Length')\n",
    "plt.ylabel('Petal Width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1e9c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(iris.data[:, 0], iris.data[:, 1], c=kmeans.labels_, alpha=0.8)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], marker=\"+\", s=1000, c=[0, 1, 2])\n",
    "plt.xlabel('Sepal Length')\n",
    "plt.ylabel('Sepal Width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abad07a",
   "metadata": {},
   "source": [
    "- The big markers \"+\" refers to the centroid of each cluster.\n",
    "- We can also fit the principal components to the KMeans algorithm. Perform K means on the new dataset below, and then find the centers and the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27fe118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from sklearn.metrics import silhouette_samples\n",
    "import numpy as np\n",
    "\n",
    "def plot_silhouette(km, x):\n",
    "    y_km = kmeans.fit_predict(x)\n",
    "    cluster_labels = np.unique(y_km)\n",
    "    n_clusters = cluster_labels.shape[0]\n",
    "    silhouette_vals = silhouette_samples(x, y_km, metric='euclidean')\n",
    "    y_ax_lower, y_ax_upper = 0, 0\n",
    "    yticks = []\n",
    "    for i, c in enumerate(cluster_labels):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster c, and sort them\n",
    "        c_silhouette_vals = silhouette_vals[y_km == c]\n",
    "        c_silhouette_vals.sort()\n",
    "\n",
    "        size_cluster_c = len(c_silhouette_vals)\n",
    "        y_ax_upper += size_cluster_c\n",
    "        color = cm.jet(i*1.0/n_clusters)\n",
    "        plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, color=color)\n",
    "\n",
    "        # Compute the new y_ax_lower for next plot\n",
    "        yticks.append((y_ax_lower + y_ax_upper) / 2)\n",
    "        y_ax_lower += size_cluster_c\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    silhouette_avg = np.mean(silhouette_vals)\n",
    "    plt.axvline(silhouette_avg, color='red', linestyle='--')\n",
    "\n",
    "    plt.yticks(yticks, cluster_labels + 1)\n",
    "    plt.title('Silhouette Analysis')\n",
    "    plt.ylabel('Cluster')\n",
    "    plt.xlabel('Silhouette coefficient')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7e449d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.set_params(n_clusters=3)\n",
    "plot_silhouette(kmeans, iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f92fc6",
   "metadata": {},
   "source": [
    "- The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette ranges from −1 to +1, where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "- If most objects have a high value, then the clustering configuration is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.\n",
    "\n",
    "### Summary of K-means\n",
    "K-means clustering algorithms require:\n",
    "1. the choice of the number of classes to be clustered, \n",
    "2. a starting cluster configuration assignment. \n",
    "3. In most cases, this is hard to determine. \n",
    "\n",
    "#### Strengths\n",
    "- Use simple principles for identifying clusters which can be explained in non-statistical terms.\n",
    "- It is fairly efficient and performs well at dividing the data into useful clusters.\n",
    "\n",
    "#### Weaknesses\n",
    "- It is less sophisticated than more recent clustering algorithms.\n",
    "- Because it uses initial random choice, it is not guaranteed to find the optimal set of clusters.\n",
    "- Require a reasonable guess for how many clusters naturally exist in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828a48a4",
   "metadata": {},
   "source": [
    "## 3.b Hierarchical Clustering (Agglomerative Clustering)\n",
    "\n",
    "**Hierarchical clustering** method is another popular clustering method which seeks to build a hierarchy of clusters. \n",
    "- It does not require the user to specify the number of clusters. Instead, it requires to measure the dissimilarity between the pairs of clusters. \n",
    "- The clusters at a higher level are created by merging clusters at the lower level:\n",
    "    - At the lowest level, each cluster contains a single observations.\n",
    "    - At the highest level, all of the data points from a single cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc4725c",
   "metadata": {},
   "source": [
    "- Two strategies for hierarchical clustering: bottom-up and top-down. \n",
    "- The approach can be summarized as:\n",
    "    1. Start with each point in its own cluster.\n",
    "    2. Identify the two clusters which are most similar and merge them.\n",
    "    3. Repeat step 2.\n",
    "    4. Ends when all data points are in a single cluster. \n",
    "\n",
    "In the lecture we show how to build a hierarchy in a bottom-up fashion.\n",
    "<img src=\"img/hierarchical_01.png\" width=600 length=700 />\n",
    "- There are some interpretative advantages to visualizing the dendrogram created from hierarchical clustering:\n",
    "    - The lower down in the dendrogram a cluster fusion occurs, the more similar the fused clusters are to each other.\n",
    "    - The higher up in the dendrogram a fusion occurs, the more dissimilar the fused groups are to each other.\n",
    "- In general, for any two observations we can inspect the dendrogram and find the location at which the groups that contain those two observations are fused together to get an idea of their dissimilarity.\n",
    "    - Be careful to consider the groups of points in the fusions within the dendrograms, not just individual points.\n",
    "<img src=\"img/hierarchical_02.png\" width=600 length=800 />\n",
    "- Begin with n observations and a distance measure of all pairwise dissimilarities. At this step, treat each of the n observations as their own clusters.\n",
    "    - For $i=n,n-1,\\dots,2$:\n",
    "        1. Evaluate all pairwise inter-cluster dissimilarities among the i clusters and fuse together the pair of clusters that are the least dissimilar.\n",
    "        2. Note the dissimilarity between the recently fused cluster pair and mark that as the associated height in the dendrogram.\n",
    "        3. Repeat the process in step 1, calculating the new pairwise inter-cluster dissimilarities among the remaining $(i - 1)$ clusters.\n",
    "- While we do not need to specify K a priori, in order to perform hierarchical clustering there are a few choices we need to make. Particularly:\n",
    "    - A dissimilarity measure.\n",
    "    - A linkage method.\n",
    "- We are already familiar with the idea of choosing a dissimilarity measure with the choice of distance metric. In many cases, it is sufficient to use the Euclidean distance.\n",
    "- A **linkage** is a measure of the dissimilarity between two group of points. So far we only define the distance between two points, but what do we do when we want to assess the similarity among two groups of points?\n",
    "- The most common types of linkage are described below.\n",
    "     - First, compute all pairwise dissimilarities between the observations in cluster A and the observations in cluster B. Then:\n",
    "      1. Complete Linkage: Maximal inter-cluster dissimilarity.\n",
    "            - Record the largest of the dissimilarities listed between members of A and of B as the overall inter-cluster dissimilarity.\n",
    "            - sensitive to outliers, yet it tends to identify clusters that are compact, somewhat spherical objects with relatively similar diameters.\n",
    "      2. Single Linkage: Minimal inter-cluster dissimilarity.\n",
    "             - Record the smallest of the dissimilarities listed between members of A and of B as the overall inter-cluster dissimilarity.\n",
    "             - not as sensitive to outliers, yet tends to identify clusters that have a chaining effect; these clusters often fail to represent intuitive groupings among our data, and the observations in the same cluster might be quite distant from one another.\n",
    "      3. Average Linkage: Mean inter-cluster dissimilarity.\n",
    "            - Record the average of the dissimilarities listed between the members of A and of B as the overall inter-cluster dissimilarity.\n",
    "            - tends to strike a balance between the pros and cons of complete linkage and single linkage.\n",
    "\n",
    "       4. Ward’s Linkage: Minimum variance method (for Euclidean distance).\n",
    "            - Minimize the variance of the clusters being merged.\n",
    "<img src=\"img/linkage.png\" width=800 length=800 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053699db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "hier = AgglomerativeClustering()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f03007f",
   "metadata": {},
   "source": [
    "**Arguments**:\n",
    "- n_clusters: The number of clusters to find. default=2\n",
    "- affinity: Metric used to compute the linkage. Can be “euclidean”, “l1”, “l2”, “manhattan”, “cosine”. If linkage is “ward”, only “euclidean” is accepted. default: “euclidean”\n",
    "- linkage: Which linkage criterion to use. The linkage criterion determines which distance to use between sets of observation. The algorithm will merge the pairs of cluster that minimize this criterion.\n",
    "    - ward minimizes the variance of the clusters being merged.\n",
    "    - average uses the average of the distances of each observation of the two sets.\n",
    "    - complete or maximum linkage uses the maximum distances between all observations of the two sets.\n",
    "\n",
    "**Arguments**:\n",
    "- The possible values of the affinity are “euclidean”, “l1”, “l2”, “manhattan”, “cosine”.\n",
    "- \"l1\" is the same as \"manhattan\", while \"l2\" is the same as \"euclidean\".\n",
    "- \"cosine\" here in python is not the same as we told previously.\n",
    "The smaller the euclidean/manhattan distance is, the closer the two observations are. The smaller the cosine is, the more far the observations are.\n",
    "So in Python, the cosine distance is redefined as :\n",
    "$$1-\\frac{\\sum_{i=1}^n x_{1i}\\times x_{2i}}{\\sqrt{||x_1||^2}\\times\\sqrt{||x_2||^2}}$$\n",
    "Now the cosine distance ranges from 0 to 2, and the smaller it is, the closer the observations are.\n",
    "- 0: two vectors point to the same direction\n",
    "- 1: perpendicular\n",
    "- 2: opposite direction\n",
    "\n",
    "#### Distance in Scikit Learn\n",
    "We can compute the different distances by using the function pairwise_distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e54645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([[1, 2], [2, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36232d4a",
   "metadata": {},
   "source": [
    "##### Euclidean Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f1b77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "pairwise_distances(a, metric='l2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c1873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "((1-2)**2 + (2-1) ** 2) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233e0537",
   "metadata": {},
   "source": [
    "##### Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ac8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_distances(a, metric='cosine') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9b0aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - (1*2 + 2*1)/(5**0.5 * 5**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a0c5e7",
   "metadata": {},
   "source": [
    "**Attributes**:\n",
    "- labels_: Cluster labels for each observation.\n",
    "- n_leaves_: Number of leaves in the hierarchical tree, which is also the number of observations.\n",
    "**Methods**:\n",
    "- fit: Fit the hierarchical clustering on the data.\n",
    "- get_params: Get parameters for this estimator.\n",
    "- set_params: Set the parameters of this estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f366a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "hier.set_params(n_clusters=3)\n",
    "hier.fit(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b027754",
   "metadata": {},
   "outputs": [],
   "source": [
    "hier.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce6251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = 8, 6\n",
    "plt.scatter(iris.data[:, 2], iris.data[:, 3], c=hier.labels_, alpha=0.8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "hier.set_params(n_clusters=3, affinity='cosine', linkage='complete')\n",
    "hier.fit(iris.data)\n",
    "label = hier.labels_\n",
    "plt.scatter(iris.data[:, 2], iris.data[:, 3], c=label, alpha=0.8)\n",
    "plt.xlabel('Expensure')\n",
    "plt.ylabel('Transfer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b9bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "hier.set_params(n_clusters=3, affinity='euclidean', linkage='average')\n",
    "hier.fit(iris.data)\n",
    "label = hier.labels_\n",
    "plt.scatter(iris.data[:, 2], iris.data[:, 3], c=label, alpha=0.8)\n",
    "plt.xlabel('Expensure')\n",
    "plt.ylabel('Transfer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f010e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import pandas as pd\n",
    "def linkage_frame(data):\n",
    "    row_clusters = linkage(data, method='complete', metric='euclidean')\n",
    "    columns = ['row label 1', 'row label 2', 'distance', 'no. items in clust.']\n",
    "    index = ['cluster %d' % (i + 1) for i in range(row_clusters.shape[0])]\n",
    "    linkage_df = pd.DataFrame(row_clusters, columns=columns, index=index)\n",
    "    return linkage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df589c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb18ad4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_df = linkage_frame(iris_df.values)\n",
    "linkage_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6c5c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dendrogram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca56dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dendr = dendrogram(linkage_df, leaf_rotation=90, leaf_font_size=8)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Euclidean distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb45a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_dendr = dendrogram(linkage_df, leaf_rotation=90, truncate_mode='level', p = 3, leaf_font_size=8)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Euclidean distance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67547ceb",
   "metadata": {},
   "source": [
    "<p><a name=\"PCA\"></a></p>\n",
    "\n",
    "# 4. Dimentional Reduction: Principal Component Analysis (PCA)\n",
    "\n",
    "**Multicollinearity** is a phenomenon in which two or more predictor variables in a multiple regression model are highly correlated, meaning that one can be predicted from the others through linear formulae with a substantial degree of accuracy.\n",
    "\n",
    "Issues:\n",
    "- The regression coefficients of highly correlated variables might be inaccurate (high model variance).\n",
    "- The estimate of one variable's impact on the dependent variable Y while controlling for the others tends to be less precise.\n",
    "- The nearly collinear variables contain similar information about the dependent variable, which may lead to overfitting.\n",
    "- The standard errors of the affected coefficients tend to be large.\n",
    "- Given a number of observations, additional dimensions spread the points out further and further from one another.\n",
    "- Sparsity becomes exponentially worse as the dimensionality of the data increases.\n",
    "- The model Supportive Vector Machine (lecture 6) takes advantage of the curse of dimensionality.\n",
    "<img src=\"img/pca_01.png\" width=500 length=700 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baf21c7",
   "metadata": {},
   "source": [
    "**Principal component analysis (PCA)** is a tool that finds a sequence of linear combinations of the variables to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.\n",
    "- Ideal input variables: \n",
    "    - Linearly uncorrelated\n",
    "    - Low-dimensional in the feature space\n",
    "## Motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60be3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading Packages\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d7da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "np.random.seed(3)\n",
    "x = np.random.rand(5)\n",
    "y1  = np.array([0.2]*5)\n",
    "y2 = np.random.rand(5) * 0.1 + 0.2\n",
    "for ax,y in zip([ax1,ax2],[y1,y2]):\n",
    "    ax.scatter(x,y)\n",
    "    ax.hlines(y=0.2,xmin=0.2,xmax=1, linestyle='--',alpha=0.3)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_ylim(-0, 1)\n",
    "    ax.set_xlim(0.2, 1)\n",
    "\n",
    "plt.subplots_adjust(left=0.1,bottom=0.1,right=1,top=0.9,wspace=0.4,hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a502bc3",
   "metadata": {},
   "source": [
    "- Left: We always do not need all the features. The y component of all the points are the same, it provides NO additional information.\n",
    "- Right: y values are restricted in a much smaller region than x values. This suggests that x component might provide more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5093bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "for i in range(len(x)):\n",
    "    ax1.plot([x[i], x[i]], [y[i], 0], color='red', alpha=0.3)\n",
    "ax1.scatter(x, y)\n",
    "ax1.scatter(x, np.zeros(len(x)), color='red')\n",
    "ax1.set_ylim(-0, 1)\n",
    "ax1.set_xlim(0.2, 1)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "\n",
    "\n",
    "for i in range(len(x)):\n",
    "    ax2.plot([x[i], 0], [y[i], y[i]], color='red', alpha=0.3)\n",
    "ax2.scatter(x, y)\n",
    "ax2.scatter(np.ones(len(x)) * 0.2, y, color='red')\n",
    "ax2.set_ylim(-0, 1)\n",
    "ax2.set_xlim(0.2, 1)\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "plt.subplots_adjust(left=0.1,bottom=0.1,right=1,top=0.9,wspace=0.4,hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8967a229",
   "metadata": {},
   "source": [
    "- One way to compare is to project the observations to the two axes.\n",
    "    - Left: For `x` axis, the projection spreads in a range around 0.3 to 0.9.\n",
    "    - Right: In contrast, `y` is restricted in a much smaller region. This suggests that `x` component might provide more information, because all the `y` components are \"about the same\".\n",
    "\n",
    "- Consider a set of 30 points in a three dimensional space. In such a scenario, we have: 20 observations and 3 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357b0dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(array):\n",
    "    data = [\n",
    "        [1, 0, 0],\n",
    "        [0, np.sqrt(3) / 2, -np.sqrt(1) / 2],\n",
    "        [0, np.sqrt(1) / 2, np.sqrt(3) / 2]]\n",
    "    rot = np.matrix(data=data).T\n",
    "    return np.array(np.matrix(array) * rot)\n",
    "\n",
    "n = 30\n",
    "np.random.seed(108)\n",
    "z = 10. * np.random.rand(n) - 5\n",
    "theta = 2 * np.pi * np.random.rand(n)\n",
    "a = 5 - np.abs(z)\n",
    "x = a / 2.5 * np.cos(theta)\n",
    "y = a / 5. * np.sin(theta)\n",
    "data = np.zeros((n, 3))\n",
    "data[:, 0] = x\n",
    "data[:, 1] = y\n",
    "data[:, 2] = z\n",
    "data = rotate(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c33aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_origin():\n",
    "    ax.scatter(0, 0, 0, marker='o', s=26, color='black', alpha=1)\n",
    "    \n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(*data.T, marker='o', s=16, c='Lightgreen', edgecolors='k', alpha=1)\n",
    "plot_origin()\n",
    "\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "ax.text(0+0.1,0+0.1,0+0.1,'origin')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_zlim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8a2431",
   "metadata": {},
   "source": [
    "- We compare the importance of each direction. Note that the chosen directions in the example are not parallel to any coordinate axis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7960797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(data)\n",
    "first = pca.components_[[0]]\n",
    "second= pca.components_[[1]]\n",
    "third = pca.components_[[2]]\n",
    "raw_data = data\n",
    "data = data - pca.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdbd557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vec(array, length, color='blue', alpha=1):\n",
    "    kwargs = dict(\n",
    "        color=color,  # color of the curve\n",
    "        linewidth=1.4,  # thickness of the line\n",
    "        # linestyle='--',  # available styles - -- -. :\n",
    "        alpha=alpha,\n",
    "    )\n",
    "    ax.plot(*zip(-array[0] * length, array[0] * length), **kwargs)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(*data.T, marker='o', s=16, c='Lightgreen', edgecolors='k', alpha=1)\n",
    "plot_origin()\n",
    "plot_vec(first, 6)\n",
    "\n",
    "arbi = np.array([[ 0.99249426,  0.05066054,  0.1113043 ]])\n",
    "plot_vec(arbi, 2, color='orange')\n",
    "\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_zlim(-3, 3)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dd0d68",
   "metadata": {},
   "source": [
    "- We project each observation orthogonally to the “orange” direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3031f724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project2vec(data, vec, id_=0, color='green', along=False):\n",
    "    pp = data[[id_]]\n",
    "    proj = (np.sum(vec*pp)*vec)\n",
    "    ax.scatter( *( proj.ravel() ), color=color, s=16)\n",
    "    kwargs = dict(\n",
    "        color=color,  # colour of the curve\n",
    "        linewidth=1.4,  # thickness of the line\n",
    "        # linestyle='--',  # available styles - -- -. :\n",
    "        alpha=0.5,\n",
    "    )\n",
    "    ax.plot(*(zip(pp[0], proj[0])), **kwargs)\n",
    "    if along:\n",
    "        along_kwargs = dict(\n",
    "            color='Dark' + color,  # colour of the curve\n",
    "            linewidth=1.4,  # thickness of the line\n",
    "            # linestyle='--',  # available styles - -- -. :\n",
    "            alpha=1,\n",
    "        )\n",
    "        ax.plot(*(zip(np.array([0,0,0]), proj[0])), **along_kwargs)\n",
    "    return np.sum(vec*pp)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(*data.T, marker='o', s=16, c='Lightgreen', edgecolors='k', alpha=1)\n",
    "plot_origin()\n",
    "plot_vec(first, 6)\n",
    "\n",
    "\n",
    "plot_vec(arbi, 2, color='orange')\n",
    "\n",
    "\n",
    "len_proj = project2vec(data, arbi, id_=13, color='orange', along=False)\n",
    "\n",
    "\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_zlim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64784585",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(*data.T, marker='o', s=16, c='Lightgreen', edgecolors='k', alpha=1)\n",
    "plot_vec(first, 6)\n",
    "\n",
    "\n",
    "plot_vec(arbi, 2, color='orange')\n",
    "\n",
    "proj_arbi = []\n",
    "for i in range(n):\n",
    "    len_proj = project2vec(data, arbi, id_=i, color='orange', along=False)\n",
    "    proj_arbi.append(len_proj)\n",
    "\n",
    "\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_zlim(-3, 3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d78f38",
   "metadata": {},
   "source": [
    "- We project each observation orthogonally to the “blue” direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f6f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(*data.T, marker='o', s=16, c='Lightgreen', edgecolors='k', alpha=1)\n",
    "plot_vec(first, 6)\n",
    "\n",
    "\n",
    "plot_vec(arbi, 2, color='orange')\n",
    "\n",
    "proj_first = []\n",
    "for i in range(n):\n",
    "    len_first = project2vec(data, first, id_=i, color='blue', along=False)\n",
    "    proj_first.append(len_first)\n",
    "\n",
    "\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_zlim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154adad",
   "metadata": {},
   "source": [
    "The \"blue\" direction above is actually the **first principal component**, which means:\n",
    "\n",
    "- it is the direction on which the projection of the observations is more widely spread than the projetion on any other direction.\n",
    "\n",
    "- being a direction (vector), it has as many components as the number of the features usually.\n",
    "\n",
    "Note that we characterized the first principal component direction, but we didn't show how it's found. We will discuss that but if you don't care about math, Python will find it for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba9e473",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "plt.scatter(proj_first, np.ones(n) * 2, color='blue')\n",
    "plt.axhline(y=2, color='blue')\n",
    "plt.scatter(proj_arbi, np.ones(n), color='orange')\n",
    "plt.axhline(y=1, color='orange')\n",
    "plt.ylim(0, 3)\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154ef2f3",
   "metadata": {},
   "source": [
    "- The statements above charaterizes the proncipal component direction. To find the principal component direction, we need to apply **linear algebra** which we will discuss later. However, if you don't care about math, Python will find it for you."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe879b5",
   "metadata": {},
   "source": [
    "- With the first loading vector (heuristically the most important one), we want to keep, for all the observations, only the information recorded in this direction.\n",
    "    - This is done by orthogonal linear projection.\n",
    "    - There are in general N (the number of samples) components for principal component.\n",
    "    - There are in general p (the number of features) components for a principal direction (the loading vector). \n",
    "    - The principal components live in the space of samples, while the principal directions live in the space of features. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2942b60a",
   "metadata": {},
   "source": [
    "### First Principal Component\n",
    "\n",
    "- The first loading vector gives us a vector of length 30. This vector is the first principal component. \n",
    "- We need 30 records for all the observations.\n",
    "- We do not use the xyz-coordinates – we use one coordinate, the first principal component. \n",
    "- The red projection can describe a certain length away from the origin along the principal direction and is a vector in the original xyz-coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10289365",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(*data.T, marker='o', s=16, c='Lightgreen', edgecolors='k', alpha=1)\n",
    "plot_origin()\n",
    "plot_vec(first, 6, alpha=0.4)\n",
    "\n",
    "proj_len = project2vec(data, first, id_=3, color='red', along=True)\n",
    "\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_zlim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a6a458",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(*data.T, marker='o', s=16, c='Lightgreen', edgecolors='k', alpha=0.2)\n",
    "plot_origin()\n",
    "plot_vec(first, 6, alpha=0.2)\n",
    "ax.scatter(*(proj_len*first[0]), marker='o', s=16, color='red', alpha=1)\n",
    "ax.plot(*(zip(np.array([0,0,0]), proj_len*first[0])), \n",
    "        color='red', linewidth=1.4, alpha=1)\n",
    "\n",
    "pca_fmt = '%.2f units away from the origin\\n along the 1st pca direction'\n",
    "pca_coord_txt = pca_fmt % proj_len \n",
    "ax.text(-0.01044 + 0.2, 1.164, -1.585, pca_coord_txt, color='green', size=12)\n",
    "\n",
    "origin_fmt = '(%.2f, %.2f, %.2f)\\nin the x-y-z coordinate)'\n",
    "origin_coord_txt = origin_fmt %tuple((proj_len*first[0]))\n",
    "ax.text(-0.01044 - 1.8, 0.964 - 1, -1.585 - 1, origin_coord_txt, color='blue', size=12)\n",
    "\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_zlim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09080b",
   "metadata": {},
   "source": [
    "### Second Principal Component\n",
    "- The information stored is about the variation of the points across the whole sample set.\n",
    "- Not all directions are born equal. \n",
    "- The first principal component provides the most information but most likely not all. \n",
    "- We remove the data information stored in the first principal component.\n",
    "- Then find the new direction (orthogonal to the original principal direction) on which the projection of the observations is most widely spread. \n",
    "- We consider the 2-D plane that is perpendicular to the first loading vector. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08c9863",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_plane(normal, color='blue', alpha=0.2, x_min=-1.5, x_max=2.5, y_min=-2.5, y_max=1.5):\n",
    "    x_min_rng = list(range(int(np.floor(x_min) + 1), 0))\n",
    "    x_max_rng = list(range(int(np.floor(x_max))))\n",
    "    y_min_rng = list(range(int(np.floor(y_min) + 1), 0))\n",
    "    y_max_rng = list(range(int(np.floor(y_max))))\n",
    "    surf_x, surf_y = np.meshgrid(\n",
    "        [x_min] + x_min_rng + x_max_rng + [x_max],\n",
    "        [y_min]+ y_min_rng + y_max_rng + [y_max])\n",
    "    surf_z = (-normal[0, 0]*surf_x - normal[0, 1]*surf_y - 0.5)* 1. / normal[0, 2]\n",
    "    ax.plot_surface(surf_x, surf_y, surf_z, color=color, alpha=0.1)\n",
    "    \n",
    "def project2plane(data, normal, id_=0, color='green', shoot=False):\n",
    "    pp = data[[id_]]\n",
    "    proj = pp - np.sum((pp * normal)) * normal\n",
    "    ax.scatter(*proj.ravel(), color=color, s=16)\n",
    "    if shoot:\n",
    "        kwargs = dict(\n",
    "            color=color,  # colour of the curve\n",
    "            linewidth=1.4,  # thickness of the line\n",
    "            # linestyle = '--',  # available styles - -- -. :\n",
    "            alpha=0.5,\n",
    "        )\n",
    "        ax.plot(*(zip(pp[0], proj[0])), **kwargs)\n",
    "    return pp - np.sum(normal * pp) * normal  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c3820",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(*data.T, marker='o', s=16, c='Lightgreen', edgecolors='k', alpha=1)\n",
    "plot_origin()\n",
    "plot_vec(first, 6, alpha=0.4)\n",
    "plot_plane(first, color='yellow', alpha=0.2, x_min=-2.5, x_max=2, y_min=-1, y_max=2)\n",
    "\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_zlim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(*data.T, marker='o', s=16, c='Lightgreen', edgecolors='k', alpha=1)\n",
    "plot_plane(first, color='yellow', alpha=0.2, x_min=-2.5, x_max=2, y_min=-1, y_max=2)\n",
    "\n",
    "proj_on_plane=[]\n",
    "for i in [5, 6]:\n",
    "    aa= project2plane(data, first, id_=i, color='blue', shoot=True )\n",
    "    proj_on_plane.append(aa)\n",
    "\n",
    "    \n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_zlim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e3f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = Axes3D(fig)\n",
    "plot_plane(first, color='yellow', alpha=0.2, x_min=-2.5, x_max=2, y_min=-1, y_max=2)\n",
    "\n",
    "for i in range(n):\n",
    "    project2plane(data, first, id_=i, color='blue')\n",
    "\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_zlim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ab456c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "plot_origin()\n",
    "plot_plane(first, color='yellow', alpha=0.2, x_min=-2.5, x_max=2, y_min=-1, y_max=2)\n",
    "plot_vec(second, 1.9, alpha=1, color='orange')\n",
    "\n",
    "for i in range(n):\n",
    "    project2plane(data, first, id_=i, color='blue')\n",
    "\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_zlim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b749d655",
   "metadata": {},
   "source": [
    "- This process can be continued to retain more and more information from the raw data. However, we remove one dimension each time when we remove the information recorded in a principal direction. \n",
    "- We cannot have more number of the principal components than the number of the original features we have. \n",
    "- This induction process always terminates within a finite step. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739feb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(*data.T, marker='o', s=16, c='Lightgreen', edgecolors='k', alpha=1)\n",
    "plot_origin()\n",
    "plot_vec(first, 6, color='blue', alpha=1)\n",
    "plot_vec(second, 1.9, color='orange', alpha=1)\n",
    "plot_vec(third, 1.2, color='m', alpha=1)\n",
    "\n",
    "ax.set_xlabel('X axis')\n",
    "ax.set_ylabel('Y axis')\n",
    "ax.set_zlabel('Z axis')\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-3, 3)\n",
    "ax.set_zlim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926ac7af",
   "metadata": {},
   "source": [
    "### The Mathematical Formulation\n",
    "\n",
    "In this section we identify all the elements we saw in the visualization with the mathematical formula. It is a good transition between the visualization and the Python code. To be compatible with the notation in Python, vectors are row vectors below.\n",
    "\n",
    "- The first (very important) step we need to do is to centralize the data. We may then assume our data $X$ is an `n` by `p` matrix (that means we have a data set of `n` observations and `p` features). The average of each column is 0.\n",
    "- We then project the data into any possible direction. A direction is represented by a unit vector $\\hat{u}$ in linear algebra, and the projection is $ X \\hat{u}^{\\text{T}} $\n",
    "\n",
    "**Question** What is the dimension of $\\hat{u}$ ? What is the dimensions of $X \\hat{u}^{\\text{T}}$ ? Why do these dimension make sense?\n",
    "\n",
    "1. We need to find the direction on which the projection of the data **spread most widely**. In the mathematical formulation, it can be stated as:\n",
    "$$\n",
    "\\text{maximize Var} ( X \\hat{u}^{\\text{T}} )\\\\\n",
    "\\text{subject to } \\| \\hat{u} \\| = 1\n",
    "$$\n",
    "    - The solution to the optimization problem above is the **first principal component direction**, denoted by $ \\phi_1 $. The projection of our data $X$ on the first principal component direction is $ Z_1 = X \\phi_1^{\\text{T}}$, which is called the **first principal component**.\n",
    "\n",
    "2.  Once the first $k-1$ principal components are found, the next one (if there is one) can be found inductively.\n",
    "    - We first remove the information about the first k-1 components from $X$ ($X_k$ denotes the resulted matrix). \n",
    "$$\n",
    "X_k =  X - \\sum_{i=1}^{k-1} X \\phi_i \\phi_i^{\\text{T}}\n",
    "$$\n",
    "    - With this matrix we solve the optimization problem again:\n",
    "$$\n",
    "\\text{maximize Var} ( X_k \\hat{u}^{\\text{T}} )\\\\\n",
    "\\text{subject to } \\| \\hat{u} \\| = 1\n",
    "$$\n",
    "    - Again the solution $\\phi_k$ is the **kth principal component direction** and the projection on this direction, $Z_k = X_k \\hat{u}^{\\text{T}}$, is called the **kth principal component**.\n",
    "    - **Note** Solving an optimization problem can be hard. In the setting of PCA, this is relatively easy. The principal component directions are (essentially) the **eigenvectors** of the covariance matrix of the data, arranged in the descending order of the eigenvalues they correspond to.\n",
    "\n",
    "### PCA Properties\n",
    "1. There are most `min(n, p)` principal components (but we often assume `p` of them).\n",
    "2. The variances of each principal components decreases:\n",
    "$$\n",
    "\\text{Var}(Z_1) ≥ \\text{Var}(Z_2) ≥ ... ≥ \\text{Var}(Z_p)\n",
    "$$\n",
    "3. The principal components $Z_1, Z_2, ..., Z_p$ are mutually uncorrelated.\n",
    "4. The principal component directions\n",
    "$\n",
    "\\phi_1, \\phi_2, ..., \\phi_p\n",
    "$\n",
    "are normalized and mutually perpendicular.\n",
    "\n",
    "### Geometrical Meaning\n",
    "- Geometrically we can imagine that the original data set sits inside $\\mathbb{R}^f$ as a high dimensional scatterplot. \n",
    "- The selection of the top p principal directions establishes a linear projection $\\mathbb{R}^f\\to\\mathbb{R}^p$ into a lower dimensional space. \n",
    "- There are many orthogonal linear projections $\\mathbb{R}^f\\to\\mathbb{R}^p$. But PCA is special that it collapses directions in which the variances are small and preserve those whose variances are larger. \n",
    "- Those directions which get collapsed are interpreted as noise of the data. \n",
    "- Even though the apparent dimension of the data is f dimensional, PCA hypothesizes that the true dimension of the data lies in a p dimensional linear space. \n",
    "- In this sense, PCA is a de-noising process revealing the true nature of the data. \n",
    "- Nonlinear projections into curved objects instead of linear spaces is called manifold learning as non-linear smooth objects are called manifolds in geometry.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6325b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27354932",
   "metadata": {},
   "source": [
    "**Arguments**: \n",
    "\n",
    "- **n_components**: Number of components to keep. In default it is `min(`n_samples, n_features`)`.\n",
    "\n",
    "**Attributes**:\n",
    "\n",
    "- **components_**: Components with maximum variance.\n",
    "- **explained\\_variance\\_ratio\\_**: Percentage of variance explained by each of the selected components. \n",
    "- **mean_**: The average of each feature.\n",
    "\n",
    "**Methods:**\n",
    "\n",
    "- **fit**: Fit the model with X.\n",
    "- **fit_transform**: Fit the model with X and apply the dimensionality reduction on X.\n",
    "- **inverse_transform**: Transform data back to its original space.\n",
    "- **get_covariance**: Compute data covariance with the generative model.\n",
    "- **get_params**: Get parameters for this estimator.\n",
    "- **set_params**: Set the parameters of this estimator.\n",
    "- **transform**: Apply the dimensionality reduction on X.\n",
    "\n",
    "Here is a simple example. Let's use the data we visualize. For visualization, we have manually centralized the data. Here we use the data without being centralized, we will see that Python take care of this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb876ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "np.random.seed(108)\n",
    "z = 10. * np.random.rand(n) - 5\n",
    "theta = 2 * np.pi * np.random.rand(n)\n",
    "a = 5 - np.abs(z)\n",
    "x = a / 2.5 * np.cos(theta)\n",
    "y = a / 5. * np.sin(theta)\n",
    "w = a / 1.25 * np.sin(theta)\n",
    "v = a / 0.75 * np.cos(theta)\n",
    "data = np.zeros((n, 5))\n",
    "data[:, 0] = x\n",
    "data[:, 1] = y\n",
    "data[:, 2] = z\n",
    "data[:, 3] = w\n",
    "data[:, 4] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b85b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax1 = Axes3D(fig)\n",
    "\n",
    "ax1.scatter(*data[:,3:5].T, marker='o', s=16, c='Lightgreen', edgecolors='k', alpha=1)\n",
    "ax1.set_xlabel('z')\n",
    "ax1.set_ylabel('w')\n",
    "ax1.set_zlabel('v')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90da0f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.set_params(n_components=None)\n",
    "pca.fit(data)\n",
    "plt.plot(range(5), pca.explained_variance_ratio_)\n",
    "plt.scatter(range(5), pca.explained_variance_ratio_)\n",
    "plt.xlabel('ith components')\n",
    "plt.ylabel('Percentage of Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e40837",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.set_params(n_components=3).fit(data)\n",
    "data2 = pca.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf60835",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de96dccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(*data2.T, marker='o', s=16, c='Lightgreen', edgecolors='k', alpha=1)\n",
    "ax.set_xlabel('PC1')\n",
    "ax.set_ylabel('PC2')\n",
    "ax.set_zlabel('PC3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb104b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.set_params(n_components=2).fit(data)\n",
    "data3 = pca.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f50de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data3[:,0],data3[:,1])\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1954a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.inverse_transform(data3) == data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee2f50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
