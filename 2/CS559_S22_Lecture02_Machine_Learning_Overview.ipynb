{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Lecture 02 - Machine Learning Project Workflow</center>\n",
    "\n",
    "## Outline\n",
    "1. Overview of ML\n",
    "    1. Introduction\n",
    "    2. ML from different persectives\n",
    "    3. Different Learnings in ML\n",
    "2. ML Project Workfolow\n",
    "    1. What makes ML so special?\n",
    "    2. What is the workflow in ML Project?\n",
    "        1. What is preprocessing and exploratory data analysis (EDA)?\n",
    "        2. How do we make models and what is after?\n",
    "        3. How can we make the models better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Overview of ML\n",
    "## 1.A Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning (ML) is everywhere in \n",
    "- There are many fields ML is highly or indirectly involved...\n",
    "    - Computer Science, Healthcare, Retail, Manufacturing, Energy, Finance, Science, Technology, etc.\n",
    "- Why and How?\n",
    "    - ML was around us for a long time.\n",
    "    - As turning into the **Big Data** era,\n",
    "        - Manuscripting models manually is limited (statistically, technically, and physically).\n",
    "        - There are just more things humans needed to learn and experience data structure, pattern, and recognition.\n",
    "        - and many more reasons... \n",
    "- What is Machine Learning?\n",
    "    - A computer program is said to learn from experience,¬†**E**,¬†with respect to some class of tasks,¬†**T**,¬†and performance measure,¬†**P**.¬†\n",
    "    - so, if its performance at tasks in¬†**T**, as measured by¬†**P**, improves with experience¬†**E**.\n",
    "    - The term first coined in 1959, by Arthur Samuel from IBM\n",
    "         - A branch of Artificial Intelligence (AI), \n",
    "         - Focused on design and development of algorithm \n",
    "        - Input: empirical data, such as that from sensors or databases, \n",
    "        - Output: **patterns** or **predictions** thought to be features of the underlying mechanism that generated the data.\n",
    "    - Learner (the algorithm):\n",
    "        - Takes advantage of **data** to capture *characteristics of interest* of their unknown underlying probability distribution. \n",
    "    - One fundamental difficulty:\n",
    "        - Generalization: The set of all possible behaviors given all possible inputs is **too large** to be included in the set of observed examples (training data). Hence the learner must **generalize** from the given examples in order to produce a useful output in new cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.B ML from different perspectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **Artificial Intelligence (AI)** View:\n",
    "    - Learning is central to human knowledge and intelligence, and likewise, it is also essential for building intelligent machines.\n",
    "    - Years of effort in AI has shown that trying to build intelligent computers by programming all the rules cannot be done; automatic learning is crucial.\n",
    "    - For example, we humans are not born with the ability to understand language. We learn it and it makes sense to try to have computers learn language instead of trying to program it all it.\n",
    "- The **Software Engineering** View: \n",
    "    - ML allows us to program computers by example, which can be easier than writing code in the traditional way.\n",
    "- The **Statistics** View:\n",
    "    - ML is the marriage of computer science and statistics: computational techniques are applied to statistical problems. \n",
    "    - ML has been applied to a vast number of problems in many contexts, beyond the typical statistics problems. \n",
    "    - ML is often designed with different considerations than statistics (e.g., speed is often more important than accuracy).\n",
    "- Examples: Spam Filtering, Face Detection, Games, etc..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/face_detection.png\" width=600 length=600/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/games.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.C Different Learnings in ML\n",
    "\n",
    "#### Supervised Learning\n",
    "- Labeled Data/targets\n",
    "- Direct Feedback\n",
    "- Predict outcome\n",
    "- Forecast future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.read_csv('Housing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DF.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Labels: headers, column names, feature names\n",
    "- Columns: features, predictors, attributes\n",
    "    - categorical and discrete data: integers, texts\n",
    "    - numerical and continous data: floats\n",
    "- Rows: observations, examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsupervised Learning\n",
    "- No labels/targets\n",
    "- No feedback\n",
    "- Find hidden structure in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Spiral.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reinforcement Learning\n",
    "- Decision Process\n",
    "- Reward system\n",
    "- Learning series of actions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/gm_example_0.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ML Project Workflow\n",
    "## 2.A What makes ML so special?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ML_approach.png\" width=500 height=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.B What is the workflow in ML project?\n",
    "ML is about: \n",
    "- Given a collections of examples, called ‚Äútraining data‚Äù\t\n",
    "- We want to predict something about novel examples, called ‚Äútest data‚Äù\n",
    "\n",
    "What we usually do:\n",
    "- Build idealized models of the application area we are working in\n",
    "- Develop algorithms and implement in code\n",
    "- Use historical data to learn numeric parameters, and sometimes model structure\n",
    "- Use test data to validate the learned model, quantitatively measure its predictions\n",
    "- Assess errors and repeat‚Ä¶\n",
    "\n",
    "Every machine learning algorithm has three components:\n",
    "- **Representation/Model Class**\n",
    "    - Decision trees\n",
    "    - Sets of rules / Logic programs\n",
    "    - Graphical models (Bayes/Markov nets)\n",
    "    - Neural networks\n",
    "    - Support vector machines\n",
    "    - Model ensembles\n",
    "- **Evaluation/Objective Function**\n",
    "    - Accuracy\n",
    "    - Precision and recall\n",
    "    - Squared error\n",
    "    - Likelihood\n",
    "    - Posterior probability\n",
    "    - Cost / Utility\n",
    "    - Margin\n",
    "    - Entropy\n",
    "    - K-L divergence\n",
    "- **Optimization**\n",
    "    - Discrete optimization \n",
    "        - Minimal Spanning Tree\n",
    "        - Shortest Path\n",
    "    - Continuous Optimization\n",
    "        - Gradient Descent\n",
    "        - Linear Programming"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/ML_roadmap.png\" width=700 length=700/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.B.a What is preprocessing and exploratory data analysis (EDA)?\n",
    "#### Importance of data preprocessing\n",
    "- Data preprocessing is to make sure we have sensible data for ML\n",
    "<img src=\"img/garbagein_garbageout.png\" width=400 length=400/>\n",
    "\n",
    "#### Missing values\n",
    "- **Observation we intended to collect but did not get them**\n",
    "    - Data entry issues, equipment errors, incorrect measurement etc\n",
    "        - An individual may only have responded to certain questions in a survey, but not all\n",
    "- **Problems of missing data**\n",
    "    - Reduce representativeness of the sample\n",
    "    - Complicating data handling and analysis\n",
    "    - Bias resulting from differences between missing and complete data\n",
    "- **Missing data handling**\n",
    "    - Reducing the data set\n",
    "        - Elimination of samples with missing values\n",
    "        - Elimination of features (columns) with missing values\n",
    "    - Imputing missing values\n",
    "        - Replace the missing value with the mean/median (numerical) or most common  (categorical) value of that feature\n",
    "    - Treating missing attribute values as a special value\n",
    "        - Treat missing value itself as a new value and be part of the data analysis\n",
    "            - Make a simple model to estimate the missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducing the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_=DF.drop('total_bedrooms',axis=1)\n",
    "DF_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_=DF.dropna()\n",
    "DF_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(DF['total_bedrooms'].mean(),2))\n",
    "print(DF['total_bedrooms'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF['total_bedrooms'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_ = DF\n",
    "X0 = DF_['total_bedrooms'].dropna()\n",
    "X1=DF_['total_bedrooms'].fillna(value=DF_['total_bedrooms'].mean())\n",
    "X2=DF_['total_bedrooms'].fillna(value=DF_['total_bedrooms'].median())\n",
    "X3=DF_['total_bedrooms'].fillna(value=0)\n",
    "print(\"X0=\",round(X0.mean(),2))\n",
    "print(\"X1=\",round(X1.mean(),2))\n",
    "print(\"X2=\",round(X2.mean(),2))\n",
    "print(\"X3=\",round(X3.mean(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [X0,X1,X2,X3]\n",
    "fig = plt.figure(figsize =(10, 7))\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "bp = ax.boxplot(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_ = DF[DF['total_bedrooms']<1100]\n",
    "X0 = DF_['total_bedrooms'].dropna()\n",
    "X1=DF_['total_bedrooms'].fillna(value=DF['total_bedrooms'].mean())\n",
    "X2=DF_['total_bedrooms'].fillna(value=DF['total_bedrooms'].median())\n",
    "X3=DF_['total_bedrooms'].fillna(value=0)\n",
    "print(\"X0=\",round(X0.mean(),2))\n",
    "print(\"X1=\",round(X1.mean(),2))\n",
    "print(\"X2=\",round(X2.mean(),2))\n",
    "print(\"X3=\",round(X3.mean(),2))\n",
    "X = [X0,X1,X2,X3]\n",
    "fig = plt.figure(figsize =(10, 7))\n",
    "ax = fig.add_axes([0, 0, 1, 1])\n",
    "bp = ax.boxplot(X)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data in different scales\n",
    "Approaches to bring different values onto the same scale\n",
    "- **Normalization**: rescale the feature to a range of (0,1)\n",
    "    $$x_{norm}^j = \\frac{x^j-x_{min}}{x_{max}-x_{min}}$$\n",
    "    where $j$ is the column number. \n",
    "    - To changes values to a common scale (between 0 and 1) without distorting differences in the ranges of values.\n",
    "    - Use when features are in different ranges. \n",
    "    - Use when distribution is not known or skewed. \n",
    "    - Use for specific ML algorithms, e.g., K-Nearest Neighbors and Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=DF['median_house_value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = MinMaxScaler()\n",
    "X_ = sc.fit_transform(np.array(X).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2,figsize=(10,5))\n",
    "ax1.hist(X)\n",
    "ax2.hist(X_)\n",
    "plt.subplots_adjust(left=0.1,bottom=0.1,right=0.9, top=0.9,wspace=0.4,hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Standardization**: re-center the feature to the mean and scaled by variance\n",
    "    $$x_{std}^j= \\frac{x^j-\\mu_{x}}{\\sigma_x}$$\n",
    "    where $\\mu_x$ is the average of $x^j$ and $\\sigma_x$ is the standard deviation of $x_j$.\n",
    "    - When measurements are in different units, we standardize the feature around the center 0 with 1ùúé.\n",
    "    - Values at different scales can cause bias. \n",
    "    - Assumes that data has a Gaussian distribution and if ML algorithm holds the assumption (e.g., Linear Regression, Logistic Regression, Linear Discriminant Analysis). \n",
    "\n",
    "- Data scaling should be one of the first steps of data preprocessing for  many machine learning algorithms\n",
    "     - Some machine learning algorithms can handle data in different scales (e.g.,  decision trees and random forests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_ = sc.fit_transform(np.array(X).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "ax1.hist(X)\n",
    "ax2.hist(X_)\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.4, \n",
    "                    hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skewed Data\n",
    "- Real-world data can be messy and contains attributes that need modifications before they can be used in modeling. \n",
    "- In case of normal distribution, the mean, median, and mode are approximately close to each other at the center of distribution. \n",
    "- The skewness of data can be determined by how these quantities are related to one another. \n",
    "    - **Right** skewed or **Positive** skewed: Mean > Median > Mode\n",
    "    - **Left** Skewed or **Negative** skewed: Mode > Median > Mean\n",
    "    - The tail region may act outliers that can affect the model‚Äôs performance in regression models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skewnorm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "numValues, maxValue = 1000,100\n",
    "skewnessL,skewnessR = -10,10   #Negative values are left skewed, positive values are right skewed.\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "for ax, skewness in zip([ax1,ax2],[-10,10]):\n",
    "    random = skewnorm.rvs(a = skewness,loc=maxValue, size=numValues)\n",
    "    ax.hist(random,30,density=True, color='red',alpha=0.3)\n",
    "    ax.vlines(np.mean(random),0,1.0,color='black',label='mean')\n",
    "    ax.vlines(np.median(random),0,1.0,color='black',linestyle='--',label='median')\n",
    "    ax.legend(loc='best')\n",
    "plt.subplots_adjust(left=0.1,bottom=0.1,right=1,top=0.9,wspace=0.4,hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handling kewness:\n",
    "    - Log transformation transforms skewed distribution to a normal distribution. (usually applies to right skewed data)\n",
    "        - Values $\\le0$ cannot be transformed. \n",
    "        - Add some constant so the minimum value be greater than $1\\to\\log{(1)}=0$ \n",
    "    - Remove outliers (both)\n",
    "    - Normalize (applies to right skewed data)\n",
    "    - Cube root, square root (applies to right skewed data)\n",
    "    - Reciprocal (applies to right skewed data)\n",
    "    - Square (applies to left skewed data)\n",
    "    - Box Cox transformation (applies to both)\n",
    "        - Transform using equations below:\n",
    "        - $y(\\lambda) = \\begin{cases}\n",
    "                (y^{\\lambda}-1)/\\lambda & \\text{if $\\lambda\\ne0$ and $y>0$} \\\\\n",
    "\\log{y} & \\text{if $\\lambda=0$ and $y>0$} \\\\\n",
    "\\end{cases}$\n",
    "        - $y(\\lambda) = \\begin{cases}\n",
    "                ((y+\\lambda_2)^{\\lambda_1}-1)/\\lambda_1 & \\text{if $\\lambda_1\\ne0$ and $y<0$} \\\\\n",
    "\\log{y+\\lambda_2} & \\text{if $\\lambda_1=0$ and $y<0$} \\\\\n",
    "\\end{cases}$\n",
    "        - Usually, $\\lambda=[‚àí5,5]$ but we use a $\\lambda$ value that gives the best approximation to a normal distribution.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random0 = X\n",
    "\n",
    "random1 = DF['median_house_value'][DF['median_house_value']<500000]\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "for ax, random in zip([ax1,ax2],[random0,random1]):\n",
    "    ax.hist(random,30,density=True, color='red',alpha=0.3)\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    ax.vlines(np.mean(random),y_min,y_max,color='black',label='mean')\n",
    "    ax.vlines(np.median(random),y_min,y_max,color='black',linestyle='--',label='median')\n",
    "    ax.text((random.max()+random.min())/2,y_max*0.90,round(random.skew(),2),ha='center')\n",
    "    ax.legend(loc='best')\n",
    "plt.subplots_adjust(left=0.1,bottom=0.1,right=1,top=0.9,wspace=0.4,hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = random1\n",
    "random2 = (X-X.min())/(X.max()-X.min())\n",
    "random3 = np.sqrt(X)\n",
    "random4 = stats.boxcox(X)[0]\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,5))\n",
    "for ax, random in zip([ax1,ax2,ax3],[random2,random3,random4]):\n",
    "    ax.hist(random,30,density=True, color='red',alpha=0.3)\n",
    "    y_min, y_max = ax.get_ylim()\n",
    "    ax.vlines(np.mean(random),y_min,y_max,color='black',label='mean')\n",
    "    ax.vlines(np.median(random),y_min,y_max,color='black',linestyle='--',label='median')\n",
    "    ax.text((random.max()+random.min())/2,y_max*0.90,round(pd.Series(random).skew(),2),ha='center')\n",
    "    ax.legend(loc='best')\n",
    "plt.subplots_adjust(left=0.1,bottom=0.1,right=1,top=0.9,wspace=0.4,hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical Data\n",
    "- for ordinal data, convert the strings into comparable integer values\n",
    "    - E.g., XL > L > M > S $\\to$ 5 (XL) > 4 (L) > 3 (M) > 2 (S)\n",
    "    - Note that the value of integer itself has no special meaning besides for ordering\n",
    "    - Mapping needs to be unique: 1 to 1 mapping for going back and forth\n",
    "- For nominal data, convert the strings into integers\n",
    "    - E.g., Red (0), Blue (1), Green (2)\n",
    "    - A common practice to avoid software glitches in handling strings\n",
    "    - Note that the value of integer itself has no special meaning (non-comparable)\n",
    "    - Mapping needs to be unique: 1 to 1 mapping for going back and forth\n",
    "- To avoid mistakenly comparing encoded integers for nominal data, one-  hot encoding can be used\n",
    "    - Each unique value becomes a separate dummy feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "housing_cat = DF[\"ocean_proximity\"]\n",
    "housing_cat_encoded = encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(housing_cat_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation between features and feature engineering\n",
    "- One good way to reduce the data size\n",
    "- Correlations between two features explains how they are related to each other. \n",
    "    - Pearson correlation coefficient is widely used. \n",
    "    $$\\rho_{x,y}=\\frac{Cov(x,y)}{\\sigma_x\\sigma_y}$$\n",
    "    - Ranges from -1 to 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = DF.corr()\n",
    "corr_matrix[['total_bedrooms','total_rooms','households']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature engineering extract features using domain knowledge\n",
    "    - Improves the performance of ML \n",
    "    - Sometimes can be considered as applied ML\n",
    "- For example, if $X$ and $Y$ are tightly correlated \n",
    "    - We can use only $X$ as an independent variable \n",
    "    - Or make a new feature call $Z = XY$ as an independent variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(DF['total_bedrooms'],DF['total_rooms'])\n",
    "plt.xlabel('total bedrooms')\n",
    "plt.ylabel('total rooms')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = DF[['total_bedrooms','total_rooms','households']].dropna()\n",
    "X['bedroom_per_room'] = X['total_bedrooms']/X['total_rooms']\n",
    "X.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.B.b How do we make models and what is after?\n",
    "Machine learning is an **algorithm** that learns a model from data (training),  so that the model can be used to predict certain properties about new  data (generalization)\n",
    "<img src=\"img/ML_Model.png\" width=600 length=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.B.c How can we make the models better?\n",
    "- **Training** is to build the ML model from data\n",
    "    - Typically, training is a one-time effort, but computationally intesive\n",
    "    - Speed is a main concern\n",
    "- **Interference** is to use the ML model to predict results for new data\n",
    "    - **Generalization** is the most interesting stage for applications\n",
    "    - Typically, inference is fast but happens more frequently with a lot of more new data (unlabled)\n",
    "    - Scalability is a main concern\n",
    "- <b>Split</b> known data into train and test datasets\n",
    "    - train dataset: a data set used to train the model\n",
    "    - test dataset: a data set used to give an indication on how well the trained model will generalize to new data (unknown at this point)\n",
    "    - test dataset is kept until the **very end** to evaluate the final model. \n",
    "    - since test dataset withholds valuable information that learning algorithm could benefit from, we do not want to put too much data into the test dataset neither. \n",
    "        - 70:30, 80:20, 90:10 splits are common\n",
    "<img src=\"img/Data_split_01.png\" width=600 length=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(DF, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set),\"train +\", len(test_set),\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>Cross-validation</b>: a model tuning process\n",
    "    - How can we make the model training process to be aware of the targeted  generalization quality so that training can do something about it?\n",
    "    - We need to put the predicted generalization results as part of the training  optimization goal\n",
    "        - We can **NOT** use the predicated generalization results from the test data,  otherwise, the test data would become part of the training process\n",
    "        - We want to keep the test data still independent of training so that its predication  can still be a good indication of generalization quality for future unknown new data\n",
    "    - <b>Holdout cross-validation</b>\n",
    "        - Training dataset is further split into two sets: training set + validation set.\n",
    "        - Validation results are sued to drive the continuation of training process until we obtain a reasonable validation result.\n",
    "        - We still use test data to report the predicted generalization quality. \n",
    "<img src=\"img/Data_split_03.png\" width=600 length=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <b>K-Fold Cross-Validation</b>\n",
    "    - Repeat holdout cross-validation k times on k subsets of the training data\n",
    "        - Randomly split the training dataset into k folds without replacement\n",
    "            - K-1 folds are used for training, and one fold used for validation\n",
    "    - Repeat this k times so that we obtain k models\n",
    "    - Typically k=10, but larger k for smaller dataset, and smaller k for larger dataset\n",
    "    - Pros: the whole dataset is used as both a training set and validation set.\n",
    "    - Cons: \n",
    "        - Not useful for imbalance datasets. \n",
    "        - Not suitable for sequential datasets. \n",
    "<img src=\"img/k_fold_CV.png\" width=600 length=600 />\n",
    "- <b>Stratified K-Fold Cross-Validation</b>\n",
    "    - An enhanced version of K-Fold CV which is mainly used for imbalanced datasets. \n",
    "    - Each fold will have the same ratio of instances of target variables as in the whole datasets. \n",
    "    - Pros: works perfectly well for imbalanced data\n",
    "    - Cons: not suitable for sequential datasets. \n",
    "- <b>Leave One Out CV</b>\n",
    "    - An exhausive CV technique in which 1 sample point is used as a validation set and the remaining n-1 samples are used as a traning set. \n",
    "    - Repeats until every sample of the dataset is used as a validation point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Example Demonstration. \n",
    "\n",
    "Data pre-processing is a very important stage in machine learning project because how well the model is trained, the meaningfulness of model is based on train data set - a model can have a stastically well done but may not tell a stroy. The preprocessing is therefore a stage where we spend most of time before we make models. The preprocessing is broadly separate into three parts: Data Cleaning, Data Transformation, and Data Reduction. We focus on exploratory data anslysis (EDA) to achieve insights and statistical measure in order to clean, tranform, and reduce the data to get ready for modeling. In this demonstration note, we will walk through to preprocessing using `Housing.csv` data. \n",
    "\n",
    "1. A Quick Look at the Data Structure.\n",
    "2. Exploring Data Analysis (EDA)\n",
    "3. Feature Engineering\n",
    "4. Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.read_csv('Housing.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 A Quick Look at the Data Structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(DF.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 10 columns - `longitude`, `latitude`, `housing_median_age`, `total_rooms`, `total_bedrooms`, `population`, `households`, `median_income`, `median_house_value`, `ocean_proximity`.\n",
    "- use `shape` to find the size of data - 20640 rows and 10 columns. We are going to treat the problem as a supervised learning problem to predict `meadian_house_value` which we will call it a **target** and the rest **features**, **predictors**, or **attributes**. We call the rows **examples** or **observations**\n",
    "- use `head` to observe how data looks is structured. However, it does not give descriptions of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- use `info` to look for number of rows, and each attribute's type and number of non-null values. \n",
    "- notice `total_bedrooms` has only 20,433 non-null values while other attributes have 20,640 non-null values. This means that `total_bedrooms` have 207 missing values or *no observations*. \n",
    "- all attributes are numerical except the `ocean_proximity`. We see that this is a text data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF[\"ocean_proximity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe()` method shows a summary of the numerical attributes. The count, mean, min, and max rows are self-explanatory. Note that the null values are ignored (see `total_bedrooms`). The std row shows the standard deviation. The 25%, 50%, and 75% rows show the corresponding *percentiles* - the value below which a given percentage of observations in a group of observation falls. \n",
    "\n",
    " We also aggregate data by the group to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.groupby(\"ocean_proximity\").agg({'mean'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We often make a boxplot for a visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "features = DF.columns.tolist()\n",
    "\n",
    "ax = DF[features[2:9]].plot(kind='box', title='boxplot', showmeans=True,figsize=(15,7))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, len(features[2:9]), figsize = (25,7),facecolor='white')\n",
    "for i,feat in zip(range(len(features[2:9])),features[2:9]):\n",
    "    axs[i].boxplot(DF[feat].dropna())\n",
    "    axs[i].set_xlabel(feat)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only `total_bedrooms` have missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few things to notice from histograms:\n",
    "- The `median_income` is not in U.S. dollars. The data has been scaled and capped at 15 for higher median incomes and at 0.5 for lower median incomes. Working with preprocessed attributes is common in ML and it is not necessarily a problem. However, it is important to understand how the data was computed. \n",
    "- The `median_house_value` and `housing_median_age` are capped at 500,000 and 50, respectively. The former may be a serious problem since it is the target attribute and ML algorithms may learn that prices never go beyond that limit. If the goal is to predict `median_house_value` under \\\\$500,000, there is no problem - we can just use observations under the condition. However, if we have to predict even beyond \\\\$500,000, there we have two options:\n",
    "    1. Collect proper labels for the districts whose labels were capped. \n",
    "    2. Remove these districts since it will poorly predict beyond \\\\$500,000 if they are included. (Do not assume that removing \\\\$500,000 observations without EDA is the right choice!)\n",
    "- These features have very different scales. \n",
    "- Many histograms are *tail heavy*. This may make it a bit harder for some ML algorithms to detect patterns. We need to transform these attributes to have more **bell-shaped distributions**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Exploring Data Analysis\n",
    "### Visualization\n",
    "\n",
    "Visualization is one of the easiest ways to understand the data structure - it is not only limited to the statistical measurements but also insights to learn about the data itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.plot(kind=\"scatter\",x=\"longitude\",y=\"latitude\", alpha=0.4, s=DF[\"population\"]/100,\n",
    "       label=\"population\",c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),colorbar=True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plots made provides us the geomatrical information of data. \n",
    "- We can see that the houses are located in California bayside. \n",
    "- We can see the poluations of data and understand which part of bayside have high and low median house value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `corr()` to compute the *standard correlation coeficient* also known as (*Pearson's r*) between every pair of features. \n",
    "- The coefficent ranges from -1 to 1. Closer to 1 means that there is a strong positievv correlation and being closer to 0 means that there is no correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = DF.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the `median_house_value` tends to go up when the `median_income` goes up.\n",
    "- the `median_house_value` tends to go down as `latitude` and `longitude` goes up. \n",
    "- the `median_income` has the strongest correlation. \n",
    "\n",
    "The correlation study brings important information that can help us to design the preprocessing. We can predict that `median income` will be a major feature in the prediction whereas `population` will have the least impact in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\n",
    "scatter_matrix(DF[features], figsize=(12,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For details, read the link: https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html\n",
    "- the most promising feature to predict the `median_house_income` is the `median_income`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = DF.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.plot(kind=\"scatter\", x=\"median_income\",y=\"median_house_value\", alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the correlation is strong. \n",
    "- there is a price cap having a horizontal line at \\\\$500,000.\n",
    "- Reveals other less obvious straight lines around \\\\$450,000, \\\\$350,000, and \\\\$280,000. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Data Mining & Feature Engineering\n",
    "### New Combinded Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following arguement:\n",
    "1. the `total_rooms` is not useful if the `household` is not known. \n",
    "2. the `total_bedrooms` itself is not useful if the `total_rooms` is not known. \n",
    "3. the arguments above does not come from ML experience, but more from how well deeply we about the data and the case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF[\"rooms_per_household\"]=DF[\"total_rooms\"]/DF[\"households\"]\n",
    "DF[\"bedrooms_per_room\"]=DF[\"total_bedrooms\"]/DF[\"total_rooms\"]\n",
    "DF[\"population_per_household\"]=DF[\"population\"]/DF[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix=DF.corr()\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"median_house_value\", \"rooms_per_household\", \"bedrooms_per_room\", \"population_per_household\"]\n",
    "scatter_matrix(DF[features], figsize=(12,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,5))\n",
    "\n",
    "n, bins, patches = ax1.hist(DF[\"rooms_per_household\"],bins=60)\n",
    "ax1.set_xlabel('rooms_per_household')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "n, bins, patches = ax2.hist(DF[\"total_rooms\"],bins=60)\n",
    "ax2.set_xlabel('total_rooms')\n",
    "ax2.set_ylabel('Frequency')\n",
    "\n",
    "n, bins, patches = ax3.hist(DF[\"households\"],bins=60)\n",
    "ax3.set_xlabel(\"households\")\n",
    "ax3.set_ylabel('Frequency')\n",
    "plt.subplots_adjust(left=0.1,bottom=0.1,right=1,top=0.9,wspace=0.4,hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `bedrooms_per_room` is much more correlated with the `median_house_value` than the `total_rooms` and the `total_bedrooms`. \n",
    "- Houses with a lower bedroom/room ratio tend to be more expensive. \n",
    "- The `rooms_per_household` is much informative than the `total_rooms`. \n",
    "- Bigger the house is, much expensive the house is. \n",
    "- This exploration does not have to be absolutely thorough - the point is to start off on the right foot and quickly gain insights that will help us to get a first reasonably good prototype. This is an iterative process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing Value Handling:\n",
    "- Impute with averge, median, any specific number (.e.g, 0), or drop \n",
    "- Use `fillna()` to fill in missing values\n",
    "- If we decided to drop, we must consider dropping the column or the rows. \n",
    "    - use `dropna()` to delete all rows for having a column with missing values. \n",
    "    - use `drop()` to delete the column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.dropna(subset=['total_bedrooms']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.drop(\"total_bedrooms\",axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median=DF[\"total_bedrooms\"].median()\n",
    "average=DF[\"total_bedrooms\"].mean()\n",
    "print(\"median is\",median)\n",
    "print(\"average is\", average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,5))\n",
    "\n",
    "n, bins, patches = ax1.hist(DF[\"total_bedrooms\"],bins=60)\n",
    "ax1.set_xlabel('Without Fillna')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "n, bins, patches = ax2.hist(DF[\"total_bedrooms\"].fillna(median),bins=60)\n",
    "ax2.set_xlabel('Fillna(median)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "\n",
    "n, bins, patches = ax3.hist(DF[\"total_bedrooms\"].fillna(average),bins=60)\n",
    "ax3.set_xlabel('Fillna(mean)')\n",
    "ax3.set_ylabel('Frequency')\n",
    "plt.subplots_adjust(left=0.1,bottom=0.1,right=1,top=0.9,wspace=0.4,hspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Kolmogorov-Smirnov test (KS-Test)** is a nonparametric test of the equality of continuous or discontinous, one-dimensional probability distributions that can be used to compare a sample with a reference probability distribution or to compare two samples. \n",
    "\n",
    "The Kolmogorov‚ÄìSmirnov statistic quantifies a distance between the empirical distribution function of the sample and the cumulative distribution function of the reference distribution, or between the empirical distribution functions of two samples. The null distribution of this statistic is calculated under the null hypothesis that the sample is drawn from the reference distribution (in the one-sample case) or that the samples are drawn from the same distribution (in the two-sample case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "ks_2samp(DF[\"total_bedrooms\"].dropna(),DF[\"total_bedrooms\"].fillna(median))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_2samp(DF[\"total_bedrooms\"].dropna(),DF[\"total_bedrooms\"].fillna(average))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the null hypothesis the two distributions are identical. If the K-S statistic is small or the p-value is high (greater than the significance level, say 5%), then we cannot reject the hypothesis that the distributions of the two samples are the same. Conversely, we can reject the null hypothesis if the p-value is low.\n",
    "- Therefore, we can either drop all `total_bedrooms` missing observations or fill either with the median or the average of `total_bedrooms`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also can use `SimpleImputer()` from `sklearn.impute` to fillin all `Nan`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "housing_num = DF.drop(\"ocean_proximity\",axis=1)\n",
    "imputer.fit(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.median().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(housing_num)\n",
    "housing_tr=pd.DataFrame(X, columns=housing_num.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_tr.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_2samp(DF[\"bedrooms_per_room\"],housing_tr[\"bedrooms_per_room\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Categorical and Text Data:\n",
    "- Most ML algorithms prefer to work with numbers. Therefore, converting text labels to numbers may bring meaningful information about data than just dropping. \n",
    "- The `ocean_proximity` is in text and we cau use `LabelEncoder()` to convert the text to numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "housing_cat = DF[\"ocean_proximity\"]\n",
    "housing_cat_encoded = encoder.fit_transform(housing_cat)\n",
    "housing_cat_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `OneHotEncoder()` encorder converts inteter categorical values into one-hot vectors. \n",
    "- Note that `fit_transform()` expects 2-D array and we need to reshape `DF_cat_encoded` because it is in 1-D array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=OneHotEncoder()\n",
    "housing_cat_encoded_1hot=encoder.fit_transform(housing_cat_encoded.reshape(-1,1))\n",
    "housing_cat_encoded_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_cat_encoded_1hot.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We get a matrix with 5 columns with 0 and 1 per row. \n",
    "- We can use `LabelBinarizer()` to do the same work in one shot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder=LabelBinarizer()\n",
    "housing_cat_encoded_1hot=encoder.fit_transform(housing_cat_encoded)\n",
    "housing_cat_encoded_1hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_Encoded=pd.DataFrame(housing_cat_encoded_1hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_Encoded.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_Encoded.columns=['<1H OCEAN','INLAND','ISLAND','NEAR BAY','NEAR OCEAN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_Encoded.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_final = pd.concat([housing_tr, housing_Encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "One of the most important transformations need to aaply to data is *feature scaling*. With few exceptions, ML algorithms do not perform well when the input numerial attributes have very different scales. \n",
    "\n",
    "Two common ways: *Min-Max Scaling* and *standarization*\n",
    "\n",
    "***Min-Max Scaling*** (**Normalization**): values are shifted and rescaled ending up from 0 to 1. Scikit-Learn provies a transformer called `MinMaxScaler`. \n",
    "\n",
    "***Standarization***: Does not have specific range values and much less affective to outliers. Scikit-Learn provies a transformer called `StandardScaler`. \n",
    "- The dataset is labeled and if we are going to predict `median_house_value`, then this is **supervised learning**. \n",
    "- If the linear models that assumes features are in Gaussian distribution such as **linear regression** is going to be used, then we need to standardize them.\n",
    "- If we are going to use other non-linear models such as tree-based algorithms (e.g., **decision tree, random forest**), then we do not need to scale the data.\n",
    "- If we are oing to classify the `ocean_proximity`, then we should concanate **housing_cat_encoded** instead. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to drop outliers for all features. \n",
    "- In the example, the outliers will be considered observations of each feature being 1 or 99 percentile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = housing_tr.columns.tolist()\n",
    "DF_ = DF_final\n",
    "for col in cols:\n",
    "    q01 = DF_[col].quantile(0.01)\n",
    "    q99 = DF_[col].quantile(0.99)\n",
    "    DF_ = DF_[(DF_[col]>q01) & (DF_[col]<q99)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DF_.shape,DF_final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = DF_final[cols].corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = DF_[cols].corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_[cols].hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_skew, right_skew,norm_ = [],[],[]\n",
    "for col in cols:\n",
    "    skew_coef = DF_[col].skew()\n",
    "    if (skew_coef<-0.05):\n",
    "        print(col,\"left skewed\",DF_[col].skew())\n",
    "        left_skew.append(col)\n",
    "    elif (skew_coef>0.05):\n",
    "        print(col,\"right skewed\",DF_[col].skew())\n",
    "        right_skew.append(col)\n",
    "    else: \n",
    "        print(col,\"close to Gaussian\")\n",
    "        norm_.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in right_skew[1:]:\n",
    "    DF_[col] = stats.boxcox(DF_[col])[0]\n",
    "    X_skew = pd.Series(DF_[col]).skew()\n",
    "    if (abs(X_skew)<=0.05):\n",
    "        print(col,X_skew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in left_skew[1:]:\n",
    "    DF_[col] = stats.boxcox(DF_[col])[0]\n",
    "    X_skew = pd.Series(DF_[col]).skew()\n",
    "    if (abs(X_skew)<=0.05):\n",
    "        print(col,X_skew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_[cols].hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_.to_csv('./Housing_scaled_01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = DF_[cols].corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(DF_final[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_scaled=pd.DataFrame(scaled_data)\n",
    "housing_scaled.columns = cols\n",
    "housing_Encoded = DF_[housing_Encoded.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_final = pd.concat([housing_scaled, housing_Encoded], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_final.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_final[cols].hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(DF[cols])\n",
    "housing_scaled=pd.DataFrame(scaled_data)\n",
    "housing_scaled.columns = cols\n",
    "housing_Encoded = DF_[housing_Encoded.columns.tolist()]\n",
    "DF_final = pd.concat([housing_scaled, housing_Encoded], axis=1)\n",
    "DF_final[cols].hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(DF_, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_set),\"train +\", len(test_set),\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `train_test_split` is the simplest function to split data. \n",
    "- `random_state`: allows to set the random generator seed, pass it multiple datasets with an identical number of rows,  and split them on the same indices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusion\n",
    "- In this lecture, we walked through the general ML project workflow. \n",
    "- The ML project is \"garbage in, garbage out\".\n",
    "- The data preprocessing is very important and it impacts the result. \n",
    "- There are many different and creative ways to attack the problem.\n",
    "    - having strong understandability of different ML algorithms and techqniques is a strong asset. \n",
    "    - due the the unlimited way of attacking the problem, being creative can bring insteresting stories of dataset. \n",
    "- There are more things to aware in preprocessing and making traning and test sets than the example we seen in this lecture. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
